===========================================
/home/swaggoner/eDNA
grtx-1.cluster
Fri Apr  5 14:33:34 EDT 2024
===========================================
13:4: not a valid test operator: (
13:4: not a valid test operator: 525.85.12

=============
== PyTorch ==
=============

NVIDIA Release 23.09 (build 69180607)
PyTorch Version 2.1.0a0+32f93b1

Container image Copyright (c) 2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

Copyright (c) 2014-2023 Facebook Inc.
Copyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)
Copyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)
Copyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)
Copyright (c) 2011-2013 NYU                      (Clement Farabet)
Copyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)
Copyright (c) 2006      Idiap Research Institute (Samy Bengio)
Copyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)
Copyright (c) 2015      Google Inc.
Copyright (c) 2015      Yangqing Jia
Copyright (c) 2013-2016 The Caffe contributors
All rights reserved.

Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

WARNING: CUDA Minor Version Compatibility mode ENABLED.
  Using driver version 525.85.12 which has support for CUDA 12.0.  This container
  was built with CUDA 12.2 and will be run in Minor Version Compatibility mode.
  CUDA Forward Compatibility is preferred over Minor Version Compatibility for use
  with this container but was unavailable:
  [[]]
  See https://docs.nvidia.com/deploy/cuda-compatibility/ for details.

NOTE: Mellanox network driver detected, but NVIDIA peer memory driver not
      detected.  Multi-node communication performance may be reduced.

2024-04-05 14:33:51.455721: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2024-04-05 14:33:52.131109: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-04-05 14:33:52.131195: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-04-05 14:33:52.134701: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-04-05 14:33:52.559581: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
0
Oversampled shape: (748, 2)


Trial 1

training set size: 780
push set size: 748
test set size: 175
train batch size: 156
test batch size: 94
Hyperparameters: {'prototype_shape': [(468, 520, 29)], 'latent_weight': [0.95], 'num_warm_epochs': [1000000], 'push_gap': [45], 'num_pushes': [2], 'last_layer_iterations': [200], 'crs_ent_weight': [1], 'clst_weight': [-1], 'sep_weight': [0.01], 'l1_weight': [0.0001], 'push_start': [37], 'p0_warm_ptype_lr': [0.05], 'p0_warm_ptype_gamma': [1], 'p0_warm_ptype_step_size': [50], 'p1_last_layer_lr': [0.001], 'p1_warm_ptype_lr': [0.05], 'p1_warm_ptype_gamma': [1], 'p1_warm_ptype_step_size': [50], 'p2_last_layer_lr': [5e-05, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5], 'p2_warm_ptype_lr': [0.05], 'p2_warm_ptype_gamma': [0.9], 'p2_warm_ptype_step_size': [50], 'p3_last_layer_lr': [0.01], 'p3_warm_ptype_lr': [0.025], 'p3_warm_ptype_gamma': [0.9], 'p3_warm_ptype_step_size': [50], 'p4_last_layer_lr': [0.007], 'p4_warm_ptype_lr': [0.02], 'p4_warm_ptype_gamma': [0.9], 'p4_warm_ptype_step_size': [50], 'joint_weight_decay': [-1], 'joint_lr_step_size': [-1], 'joint_gamma': [-1], 'joint_optimizer_lrs': [{'features': -1, 'prototype_vectors': -1}]}


Checking hyperparameters: {'p2_last_layer_lr': [5e-05, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5]}


Performing combination number 5


Exploring 9 hyperparameter combinations for grid search.


Attempting combination 6/9:
prototype_shape: (468, 520, 29)
latent_weight: 0.95
num_warm_epochs: 1000000
push_gap: 45
num_pushes: 2
last_layer_iterations: 200
crs_ent_weight: 1
clst_weight: -1
sep_weight: 0.01
l1_weight: 0.0001
push_start: 37
p0_warm_ptype_lr: 0.05
p0_warm_ptype_gamma: 1
p0_warm_ptype_step_size: 50
p1_last_layer_lr: 0.001
p1_warm_ptype_lr: 0.05
p1_warm_ptype_gamma: 1
p1_warm_ptype_step_size: 50
p2_last_layer_lr: 0.01
p2_warm_ptype_lr: 0.05
p2_warm_ptype_gamma: 0.9
p2_warm_ptype_step_size: 50
p3_last_layer_lr: 0.01
p3_warm_ptype_lr: 0.025
p3_warm_ptype_gamma: 0.9
p3_warm_ptype_step_size: 50
p4_last_layer_lr: 0.007
p4_warm_ptype_lr: 0.02
p4_warm_ptype_gamma: 0.9
p4_warm_ptype_step_size: 50
joint_weight_decay: -1
joint_lr_step_size: -1
joint_gamma: -1
joint_optimizer_lrs: {'features': -1, 'prototype_vectors': -1}



End epoch: 126
Val acc before epoch 0: 0.0
Warm optimizer lr: 0.05
Val acc before epoch 1: 0.3723404255319149
Warm optimizer lr: 0.05
Val acc before epoch 2: 0.7340425531914894
Warm optimizer lr: 0.05
Val acc before epoch 3: 0.8404255319148937
Warm optimizer lr: 0.05
Val acc before epoch 4: 0.8723404255319149
Warm optimizer lr: 0.05
Val acc before epoch 5: 0.8617021276595744
Warm optimizer lr: 0.05
Val acc before epoch 6: 0.8617021276595744
Warm optimizer lr: 0.05
Val acc before epoch 7: 0.8936170212765957
Warm optimizer lr: 0.05
Val acc before epoch 8: 0.8829787234042553
Warm optimizer lr: 0.05
Val acc before epoch 9: 0.8936170212765957
Warm optimizer lr: 0.05
Val acc before epoch 10: 0.9148936170212766
Warm optimizer lr: 0.05
Val acc before epoch 11: 0.8936170212765957
Warm optimizer lr: 0.05
Val acc before epoch 12: 0.9148936170212766
Warm optimizer lr: 0.05
Val acc before epoch 13: 0.925531914893617
Warm optimizer lr: 0.05
Val acc before epoch 14: 0.8936170212765957
Warm optimizer lr: 0.05
Val acc before epoch 15: 0.9042553191489362
Warm optimizer lr: 0.05
Val acc before epoch 16: 0.9361702127659575
Warm optimizer lr: 0.05
Val acc before epoch 17: 0.8936170212765957
Warm optimizer lr: 0.05
Val acc before epoch 18: 0.9361702127659575
Warm optimizer lr: 0.05
Val acc before epoch 19: 0.925531914893617
Warm optimizer lr: 0.05
Val acc before epoch 20: 0.8829787234042553
Warm optimizer lr: 0.05
Val acc before epoch 21: 0.925531914893617
Warm optimizer lr: 0.05
Val acc before epoch 22: 0.925531914893617
Warm optimizer lr: 0.05
Val acc before epoch 23: 0.8723404255319149
Warm optimizer lr: 0.05
Val acc before epoch 24: 0.9148936170212766
Warm optimizer lr: 0.05
Val acc before epoch 25: 0.9574468085106383
Warm optimizer lr: 0.05
Val acc before epoch 26: 0.9361702127659575
Warm optimizer lr: 0.05
Val acc before epoch 27: 0.925531914893617
Warm optimizer lr: 0.05
Val acc before epoch 28: 0.925531914893617
Warm optimizer lr: 0.05
Val acc before epoch 29: 0.9148936170212766
Warm optimizer lr: 0.05
Val acc before epoch 30: 0.9361702127659575
Warm optimizer lr: 0.05
Val acc before epoch 31: 0.9361702127659575
Warm optimizer lr: 0.05
Val acc before epoch 32: 0.9361702127659575
Warm optimizer lr: 0.05
Val acc before epoch 33: 0.9574468085106383
Warm optimizer lr: 0.05
Val acc before epoch 34: 0.9574468085106383
Warm optimizer lr: 0.05
Val acc before epoch 35: 0.9148936170212766
Warm optimizer lr: 0.05
Val acc before epoch 36: 0.9468085106382979
Warm optimizer lr: 0.05
Val acc before epoch 37: 0.925531914893617
Push epoch at epoch 38.
Final validation accuracy before push: 0.925531914893617
Push number 1





After push, before retraining last layer:
	Train acc: 0.8141025641025641
	Train Cluster: 0.6409047245979309
	Train Separation: 0.5011334002017975
(Directly after push 2) Val acc at iteration 0: 0.7021276595744681
Retraining last layer: 
Last layer lr: 0.001
	Val acc at iteration 0: 0.6914893617021277
	Val acc at iteration 1: 0.7340425531914894
	Val acc at iteration 2: 0.7553191489361702
	Val acc at iteration 3: 0.7021276595744681
	Val acc at iteration 4: 0.7553191489361702
	Val acc at iteration 5: 0.7127659574468085
	Val acc at iteration 6: 0.7340425531914894
	Val acc at iteration 7: 0.7446808510638298
	Val acc at iteration 8: 0.8085106382978723
	Val acc at iteration 9: 0.7553191489361702
	Val acc at iteration 10: 0.7659574468085106
	Val acc at iteration 11: 0.7340425531914894
	Val acc at iteration 12: 0.7553191489361702
	Val acc at iteration 13: 0.7553191489361702
	Val acc at iteration 14: 0.648936170212766
	Val acc at iteration 15: 0.7446808510638298
	Val acc at iteration 16: 0.7446808510638298
	Val acc at iteration 17: 0.723404255319149
	Val acc at iteration 18: 0.6808510638297872
	Val acc at iteration 19: 0.7021276595744681
	Val acc at iteration 20: 0.7340425531914894
	Val acc at iteration 21: 0.7553191489361702
	Val acc at iteration 22: 0.7446808510638298
	Val acc at iteration 23: 0.7553191489361702
	Val acc at iteration 24: 0.7340425531914894
	Val acc at iteration 25: 0.7127659574468085
	Val acc at iteration 26: 0.7446808510638298
	Val acc at iteration 27: 0.851063829787234
	Val acc at iteration 28: 0.7872340425531915
	Val acc at iteration 29: 0.776595744680851
	Val acc at iteration 30: 0.7553191489361702
	Val acc at iteration 31: 0.7446808510638298
	Val acc at iteration 32: 0.7872340425531915
	Val acc at iteration 33: 0.7553191489361702
	Val acc at iteration 34: 0.776595744680851
	Val acc at iteration 35: 0.7659574468085106
	Val acc at iteration 36: 0.776595744680851
	Val acc at iteration 37: 0.776595744680851
	Val acc at iteration 38: 0.7127659574468085
	Val acc at iteration 39: 0.7340425531914894
	Val acc at iteration 40: 0.7446808510638298
	Val acc at iteration 41: 0.7446808510638298
	Val acc at iteration 42: 0.7446808510638298
	Val acc at iteration 43: 0.7978723404255319
	Val acc at iteration 44: 0.776595744680851
	Val acc at iteration 45: 0.7553191489361702
	Val acc at iteration 46: 0.723404255319149
	Val acc at iteration 47: 0.7553191489361702
	Val acc at iteration 48: 0.7446808510638298
	Val acc at iteration 49: 0.7978723404255319
	Val acc at iteration 50: 0.776595744680851
	Val acc at iteration 51: 0.7446808510638298
	Val acc at iteration 52: 0.7446808510638298
	Val acc at iteration 53: 0.8085106382978723
	Val acc at iteration 54: 0.7553191489361702
	Val acc at iteration 55: 0.7872340425531915
	Val acc at iteration 56: 0.8191489361702128
	Val acc at iteration 57: 0.776595744680851
	Val acc at iteration 58: 0.8297872340425532
	Val acc at iteration 59: 0.8404255319148937
	Val acc at iteration 60: 0.723404255319149
	Val acc at iteration 61: 0.776595744680851
	Val acc at iteration 62: 0.723404255319149
	Val acc at iteration 63: 0.8617021276595744
	Val acc at iteration 64: 0.8297872340425532
	Val acc at iteration 65: 0.8297872340425532
	Val acc at iteration 66: 0.8085106382978723
	Val acc at iteration 67: 0.7659574468085106
	Val acc at iteration 68: 0.7553191489361702
	Val acc at iteration 69: 0.8191489361702128
	Val acc at iteration 70: 0.7978723404255319
	Val acc at iteration 71: 0.8085106382978723
	Val acc at iteration 72: 0.7872340425531915
	Val acc at iteration 73: 0.8297872340425532
	Val acc at iteration 74: 0.7872340425531915
	Val acc at iteration 75: 0.7659574468085106
	Val acc at iteration 76: 0.7659574468085106
	Val acc at iteration 77: 0.7872340425531915
	Val acc at iteration 78: 0.7659574468085106
	Val acc at iteration 79: 0.8085106382978723
	Val acc at iteration 80: 0.7553191489361702
	Val acc at iteration 81: 0.7978723404255319
	Val acc at iteration 82: 0.8404255319148937
	Val acc at iteration 83: 0.851063829787234
	Val acc at iteration 84: 0.7978723404255319
	Val acc at iteration 85: 0.8404255319148937
	Val acc at iteration 86: 0.7659574468085106
	Val acc at iteration 87: 0.8191489361702128
	Val acc at iteration 88: 0.8191489361702128
	Val acc at iteration 89: 0.7978723404255319
	Val acc at iteration 90: 0.8404255319148937
	Val acc at iteration 91: 0.7978723404255319
	Val acc at iteration 92: 0.8085106382978723
	Val acc at iteration 93: 0.7872340425531915
	Val acc at iteration 94: 0.7872340425531915
	Val acc at iteration 95: 0.7553191489361702
	Val acc at iteration 96: 0.7978723404255319
	Val acc at iteration 97: 0.7978723404255319
	Val acc at iteration 98: 0.7978723404255319
	Val acc at iteration 99: 0.7872340425531915
	Val acc at iteration 100: 0.776595744680851
	Val acc at iteration 101: 0.8297872340425532
	Val acc at iteration 102: 0.7659574468085106
	Val acc at iteration 103: 0.851063829787234
	Val acc at iteration 104: 0.7659574468085106
	Val acc at iteration 105: 0.8829787234042553
	Val acc at iteration 106: 0.8404255319148937
	Val acc at iteration 107: 0.8297872340425532
	Val acc at iteration 108: 0.8191489361702128
	Val acc at iteration 109: 0.8404255319148937
	Val acc at iteration 110: 0.8404255319148937
	Val acc at iteration 111: 0.7978723404255319
	Val acc at iteration 112: 0.8085106382978723
	Val acc at iteration 113: 0.8404255319148937
	Val acc at iteration 114: 0.8297872340425532
	Val acc at iteration 115: 0.8297872340425532
	Val acc at iteration 116: 0.7872340425531915
	Val acc at iteration 117: 0.7659574468085106
	Val acc at iteration 118: 0.851063829787234
	Val acc at iteration 119: 0.8297872340425532
	Val acc at iteration 120: 0.8723404255319149
	Val acc at iteration 121: 0.8404255319148937
	Val acc at iteration 122: 0.8404255319148937
	Val acc at iteration 123: 0.8297872340425532
	Val acc at iteration 124: 0.8191489361702128
	Val acc at iteration 125: 0.8085106382978723
	Val acc at iteration 126: 0.8617021276595744
	Val acc at iteration 127: 0.851063829787234
	Val acc at iteration 128: 0.8297872340425532
	Val acc at iteration 129: 0.776595744680851
	Val acc at iteration 130: 0.7978723404255319
	Val acc at iteration 131: 0.8191489361702128
	Val acc at iteration 132: 0.7872340425531915
	Val acc at iteration 133: 0.8617021276595744
	Val acc at iteration 134: 0.8404255319148937
	Val acc at iteration 135: 0.8191489361702128
	Val acc at iteration 136: 0.8085106382978723
	Val acc at iteration 137: 0.8297872340425532
	Val acc at iteration 138: 0.851063829787234
	Val acc at iteration 139: 0.7872340425531915
	Val acc at iteration 140: 0.7978723404255319
	Val acc at iteration 141: 0.851063829787234
	Val acc at iteration 142: 0.8936170212765957
	Val acc at iteration 143: 0.8085106382978723
	Val acc at iteration 144: 0.7446808510638298
	Val acc at iteration 145: 0.7978723404255319
	Val acc at iteration 146: 0.851063829787234
	Val acc at iteration 147: 0.776595744680851
	Val acc at iteration 148: 0.8723404255319149
	Val acc at iteration 149: 0.8829787234042553
	Val acc at iteration 150: 0.8191489361702128
	Val acc at iteration 151: 0.851063829787234
	Val acc at iteration 152: 0.7978723404255319
	Val acc at iteration 153: 0.8617021276595744
	Val acc at iteration 154: 0.8617021276595744
	Val acc at iteration 155: 0.8829787234042553
	Val acc at iteration 156: 0.8404255319148937
	Val acc at iteration 157: 0.8191489361702128
	Val acc at iteration 158: 0.8297872340425532
	Val acc at iteration 159: 0.7446808510638298
	Val acc at iteration 160: 0.7872340425531915
	Val acc at iteration 161: 0.8404255319148937
	Val acc at iteration 162: 0.8085106382978723
	Val acc at iteration 163: 0.8297872340425532
	Val acc at iteration 164: 0.851063829787234
	Val acc at iteration 165: 0.7872340425531915
	Val acc at iteration 166: 0.8191489361702128
	Val acc at iteration 167: 0.7978723404255319
	Val acc at iteration 168: 0.8617021276595744
	Val acc at iteration 169: 0.9042553191489362
Traceback (most recent call last):
  File "/home/swaggoner/eDNA/protopnet/main.py", line 806, in <module>
    val_acc = metrics.accuracy_score(val_actual, val_predicted)
  File "/home/swaggoner/eDNA/protopnet/main.py", line 57, in conditionally_save_model
    torch.save(obj=model, f=new_model_path)
  File "/usr/local/lib/python3.10/dist-packages/torch/serialization.py", line 620, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/usr/local/lib/python3.10/dist-packages/torch/serialization.py", line 494, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/usr/local/lib/python3.10/dist-packages/torch/serialization.py", line 465, in __init__
    super().__init__(torch._C.PyTorchFileWriter(self.name))
RuntimeError: Parent directory ./ppn_saved_models does not exist.
